{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs5T8ELFcsPqUsHhnY71Th",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samarendra-1/PW_SKILLS_ASSIGNMENTS/blob/main/Forward_and_Backward_Propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ekbnkXuUnSN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Forward propagation is the process by which a neural network computes its output predictions based on given input data. It involves passing the input through each layer of the network, performing a series of calculations (such as weighted sums and activation functions), until the output is produced.\n",
        "\n",
        "Here’s a step-by-step breakdown of forward propagation:\n",
        "\n",
        "1. Input Layer\n",
        "The data is fed into the input layer of the neural network. Each input feature corresponds to a node in this layer.\n",
        "No computations are performed at this stage; the input is simply passed forward to the next layer.\n",
        "2. Weighted Sum\n",
        "For each node (or neuron) in the next layer, the network computes a weighted sum of the inputs:\n",
        "𝑧\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "𝑤\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        "z=\n",
        "i\n",
        "∑\n",
        "​\n",
        " w\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "i\n",
        "​\n",
        " +b\n",
        "where:\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  is the input value.\n",
        "𝑤\n",
        "𝑖\n",
        "w\n",
        "i\n",
        "​\n",
        "  is the weight associated with the input.\n",
        "𝑏\n",
        "b is the bias term.\n",
        "𝑧\n",
        "z is the resulting value before applying the activation function.\n",
        "3. Activation Function\n",
        "The weighted sum (\n",
        "𝑧\n",
        "z) is passed through an activation function to introduce non-linearity and allow the network to learn complex patterns.\n",
        "Common activation functions include:\n",
        "Sigmoid:\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "ReLU:\n",
        "ReLU\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "max\n",
        "⁡\n",
        "(\n",
        "0\n",
        ",\n",
        "𝑧\n",
        ")\n",
        "ReLU(z)=max(0,z)\n",
        "Tanh:\n",
        "tanh\n",
        "⁡\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝑧\n",
        "−\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "𝑒\n",
        "𝑧\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "tanh(z)=\n",
        "e\n",
        "z\n",
        " +e\n",
        "−z\n",
        "\n",
        "e\n",
        "z\n",
        " −e\n",
        "−z\n",
        "\n",
        "​\n",
        "\n",
        "4. Hidden Layers\n",
        "The outputs from the previous layer (after applying the activation function) are passed as inputs to the next layer.\n",
        "This process (weighted sum and activation) repeats for each hidden layer in the network.\n",
        "5. Output Layer\n",
        "The final layer of the network produces the output.\n",
        "For regression tasks, the output might be a single continuous value.\n",
        "For classification tasks, the output could be probabilities (using a softmax activation) or binary labels (using a sigmoid activation).\n",
        "Example:\n",
        "For a simple neural network with one hidden layer:\n",
        "\n",
        "Input data\n",
        "𝑥\n",
        "x is multiplied by the weights (\n",
        "𝑊\n",
        "1\n",
        "W\n",
        "1\n",
        "​\n",
        " ) of the first layer, and a bias (\n",
        "𝑏\n",
        "1\n",
        "b\n",
        "1\n",
        "​\n",
        " ) is added.\n",
        "𝑧\n",
        "1\n",
        "=\n",
        "𝑊\n",
        "1\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "z\n",
        "1\n",
        "​\n",
        " =W\n",
        "1\n",
        "​\n",
        " ⋅x+b\n",
        "1\n",
        "​\n",
        "\n",
        "Apply an activation function (e.g., ReLU) to get\n",
        "𝑎\n",
        "1\n",
        "a\n",
        "1\n",
        "​\n",
        " :\n",
        "𝑎\n",
        "1\n",
        "=\n",
        "ReLU\n",
        "(\n",
        "𝑧\n",
        "1\n",
        ")\n",
        "a\n",
        "1\n",
        "​\n",
        " =ReLU(z\n",
        "1\n",
        "​\n",
        " )\n",
        "Repeat for the output layer:\n",
        "𝑧\n",
        "2\n",
        "=\n",
        "𝑊\n",
        "2\n",
        "⋅\n",
        "𝑎\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "z\n",
        "2\n",
        "​\n",
        " =W\n",
        "2\n",
        "​\n",
        " ⋅a\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        "\n",
        "𝑦\n",
        "=\n",
        "softmax\n",
        "(\n",
        "𝑧\n",
        "2\n",
        ")\n",
        "y=softmax(z\n",
        "2\n",
        "​\n",
        " )\n"
      ],
      "metadata": {
        "id": "kjGdASAMVZ08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\n",
        "\n",
        " 1. Introduce Non-Linearity\n",
        "\n",
        "Neural networks are built to solve complex problems, but a model made up entirely of linear operations (e.g., weighted sums) can only represent linear relationships.\n",
        "The activation function adds non-linearity, allowing the network to approximate complex, non-linear mappings between inputs and outputs.\n",
        "\n",
        "2. Enable Learning of Complex Patterns\n",
        "\n",
        "Real-world data often exhibits highly non-linear patterns (e.g., image recognition, speech processing).\n",
        "Activation functions help the network learn these patterns by enabling non-linear transformations at each layer.\n",
        "\n",
        "3. Allow Hierarchical Feature Learning\n",
        "\n",
        "By stacking multiple layers with activation functions, the network can learn a hierarchy of features.\n",
        "Lower layers might detect simple patterns (e.g., edges in images).\n",
        "Higher layers can combine these patterns to recognize more complex structures (e.g., faces in images).\n",
        "\n",
        "4. Bound or Transform Outputs\n",
        "\n",
        "Activation functions can bound the output of a neuron, making it easier to interpret or process.\n",
        "For example, the sigmoid function squashes outputs between\n",
        "0\n",
        "0 and\n",
        "1\n",
        "1, which is useful for probabilities.\n",
        "Some functions normalize the data or maintain certain mathematical properties required for specific tasks (e.g., softmax for classification)."
      ],
      "metadata": {
        "id": "lKdqnkmRVgMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without Activation Functions:\n",
        "If activation functions were absent:\n",
        "\n",
        "The neural network would consist of only linear transformations (matrix multiplication and addition).\n",
        "\n",
        "Stacking multiple layers would not add any additional power because a series of linear transformations is mathematically equivalent to a single linear transformation.\n",
        "\n",
        "In other words, the network would not be able to solve problems with non-linear boundaries.\n"
      ],
      "metadata": {
        "id": "d7WFOhSuV3ZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\n",
        "\n",
        "1. Forward Propagation\n",
        "\n",
        "Pass the input data through the network layer by layer to compute the predicted output (\n",
        "𝑦\n",
        "pred\n",
        "y\n",
        "pred\n",
        "​\n",
        " ).\n",
        "Compute the loss (\n",
        "𝐿\n",
        "L) using the chosen loss function (e.g., mean squared error, cross-entropy) by comparing the predicted output with the true labels (\n",
        "𝑦\n",
        "true\n",
        "y\n",
        "true\n",
        "​\n",
        " ).\n",
        "\n",
        "2. Compute the Loss Gradient\n",
        "\n",
        "Calculate the gradient of the loss with respect to the output of the network (\n",
        "∂\n",
        "𝐿\n",
        "/\n",
        "∂\n",
        "𝑦\n",
        "pred\n",
        "∂L/∂y\n",
        "pred\n",
        "​\n",
        " ).\n",
        "This measures how much the loss changes with a small change in the predicted output.\n",
        "\n",
        "3. Backward Pass: Propagate Gradients\n",
        "\n",
        "Layer-by-layer gradient computation:\n",
        "\n",
        "Starting from the output layer, compute the gradients of the loss with respect to the weights, biases, and inputs of each layer using the chain rule of calculus.\n",
        "For a given layer\n",
        "𝑙\n",
        "l:\n",
        "∂\n",
        "𝐿\n",
        "∂\n",
        "𝑊\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "=\n",
        "∂\n",
        "𝐿\n",
        "∂\n",
        "𝑧\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "⋅\n",
        "∂\n",
        "𝑧\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "∂\n",
        "𝑊\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "∂W\n",
        "(l)\n",
        "\n",
        "∂L\n",
        "​\n",
        " =\n",
        "∂z\n",
        "(l)\n",
        "\n",
        "∂L\n",
        "​\n",
        " ⋅\n",
        "∂W\n",
        "(l)\n",
        "\n",
        "∂z\n",
        "(l)\n",
        "\n",
        "​\n",
        "\n",
        "where:\n",
        "𝑧\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "=\n",
        "𝑊\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "𝑎\n",
        "(\n",
        "𝑙\n",
        "−\n",
        "1\n",
        ")\n",
        "+\n",
        "𝑏\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "z\n",
        "(l)\n",
        " =W\n",
        "(l)\n",
        " a\n",
        "(l−1)\n",
        " +b\n",
        "(l)\n",
        "  (weighted sum).\n",
        "𝑎\n",
        "(\n",
        "𝑙\n",
        "−\n",
        "1\n",
        ")\n",
        "a\n",
        "(l−1)\n",
        "  is the activation from the previous layer.\n",
        "Gradients of activation functions:\n",
        "\n",
        "Backpropagation also involves computing the derivative of the activation function for each layer.\n",
        "For example, if the activation is ReLU, its derivative is:\n",
        "∂\n",
        "ReLU\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "∂\n",
        "𝑧\n",
        "=\n",
        "{\n",
        "1\n",
        "if\n",
        "𝑧\n",
        ">\n",
        "0\n",
        ",\n",
        "0\n",
        "if\n",
        "𝑧\n",
        "≤\n",
        "0.\n",
        "∂z\n",
        "∂ReLU(z)\n",
        "​\n",
        " ={\n",
        "1\n",
        "0\n",
        "​\n",
        "  \n",
        "if z>0,\n",
        "if z≤0.\n",
        "​\n",
        "\n",
        "Repeat this process for each layer in reverse order, moving from the output layer back to the input layer.\n",
        "\n",
        "4. Update Parameters\n",
        "Use the computed gradients to update the weights and biases using a gradient descent algorithm:\n",
        "For weights:\n",
        "𝑊\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "←\n",
        "𝑊\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "−\n",
        "𝜂\n",
        "⋅\n",
        "∂\n",
        "𝐿\n",
        "∂\n",
        "𝑊\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "W\n",
        "(l)\n",
        " ←W\n",
        "(l)\n",
        " −η⋅\n",
        "∂W\n",
        "(l)\n",
        "\n",
        "∂L\n",
        "​\n",
        "\n",
        "For biases:\n",
        "𝑏\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "←\n",
        "𝑏\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "−\n",
        "𝜂\n",
        "⋅\n",
        "∂\n",
        "𝐿\n",
        "∂\n",
        "𝑏\n",
        "(\n",
        "𝑙\n",
        ")\n",
        "b\n",
        "(l)\n",
        " ←b\n",
        "(l)\n",
        " −η⋅\n",
        "∂b\n",
        "(l)\n",
        "\n",
        "∂L\n",
        "​\n",
        "\n",
        "Here,\n",
        "𝜂\n",
        "η is the learning rate, which controls the step size for parameter updates.\n",
        "5. Repeat for Multiple Epochs\n",
        "Perform forward and backward propagation repeatedly for multiple epochs over the dataset.\n",
        "Each iteration reduces the loss, gradually improving the model's performance.\n",
        "Example of Backpropagation:\n",
        "Suppose we have a simple 2-layer network:\n",
        "\n",
        "Layer 1:\n",
        "\n",
        "𝑧\n",
        "(\n",
        "1\n",
        ")\n",
        "=\n",
        "𝑊\n",
        "(\n",
        "1\n",
        ")\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "(\n",
        "1\n",
        ")\n",
        ",\n",
        "𝑎\n",
        "(\n",
        "1\n",
        ")\n",
        "=\n",
        "ReLU\n",
        "(\n",
        "𝑧\n",
        "(\n",
        "1\n",
        ")\n",
        ")\n",
        "z\n",
        "(1)\n",
        " =W\n",
        "(1)\n",
        " x+b\n",
        "(1)\n",
        " ,a\n",
        "(1)\n",
        " =ReLU(z\n",
        "(1)\n",
        " )\n",
        "Output Layer:\n",
        "\n",
        "𝑧\n",
        "(\n",
        "2\n",
        ")\n",
        "=\n",
        "𝑊\n",
        "(\n",
        "2\n",
        ")\n",
        "𝑎\n",
        "(\n",
        "1\n",
        ")\n",
        "+\n",
        "𝑏\n",
        "(\n",
        "2\n",
        ")\n",
        ",\n",
        "𝑦\n",
        "pred\n",
        "=\n",
        "softmax\n",
        "(\n",
        "𝑧\n",
        "(\n",
        "2\n",
        ")\n",
        ")\n",
        "z\n",
        "(2)\n",
        " =W\n",
        "(2)\n",
        " a\n",
        "(1)\n",
        " +b\n",
        "(2)\n",
        " ,y\n",
        "pred\n",
        "​\n",
        " =softmax(z\n",
        "(2)\n",
        " )\n",
        "Compute the loss\n",
        "𝐿\n",
        "L (e.g., cross-entropy loss).\n",
        "Backpropagate:\n",
        "Calculate\n",
        "∂\n",
        "𝐿\n",
        "/\n",
        "∂\n",
        "𝑧\n",
        "(\n",
        "2\n",
        ")\n",
        "∂L/∂z\n",
        "(2)\n",
        " .\n",
        "Propagate to\n",
        "∂\n",
        "𝐿\n",
        "/\n",
        "∂\n",
        "𝑊\n",
        "(\n",
        "2\n",
        ")\n",
        "∂L/∂W\n",
        "(2)\n",
        "  and\n",
        "∂\n",
        "𝐿\n",
        "/\n",
        "∂\n",
        "𝑎\n",
        "(\n",
        "1\n",
        ")\n",
        "∂L/∂a\n",
        "(1)\n",
        " .\n",
        "Continue propagating to Layer 1.\n",
        "Key Mathematical Concepts:\n",
        "Chain Rule: Essential for propagating gradients through multiple layers.\n",
        "If\n",
        "𝑓\n",
        "=\n",
        "𝑔\n",
        "(\n",
        "ℎ\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "f=g(h(x)), the derivative is:\n",
        "𝑑\n",
        "𝑓\n",
        "𝑑\n",
        "𝑥\n",
        "=\n",
        "𝑑\n",
        "𝑔\n",
        "𝑑\n",
        "ℎ\n",
        "⋅\n",
        "𝑑\n",
        "ℎ\n",
        "𝑑\n",
        "𝑥\n",
        ".\n",
        "dx\n",
        "df\n",
        "​\n",
        " =\n",
        "dh\n",
        "dg\n",
        "​\n",
        " ⋅\n",
        "dx\n",
        "dh\n",
        "​\n",
        " .\n",
        "Partial Derivatives: Used to compute gradients for weights and biases.\n"
      ],
      "metadata": {
        "id": "zvtZcEjEV6GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4..\n",
        "\n",
        "Backpropagation and the Chain Rule\n",
        "\n",
        "Backpropagation is the central algorithm in training neural networks. Its primary goal is to adjust the network's weights and biases to minimize the difference between predicted and actual outputs – essentially, to make the network learn. This adjustment process relies heavily on the chain rule of calculus.\n",
        "\n",
        "Why the Chain Rule?\n",
        "\n",
        "Nested Functions: A neural network is a complex structure of interconnected layers, each performing a specific function. These layers are composed of nested functions. To update the weights and biases effectively, we need to know how a change in one weight or bias affects the final output (loss). The chain rule helps us unravel this complex relationship.\n",
        "Gradient Calculation: The core of backpropagation is calculating the gradient of the loss function with respect to each weight and bias. The gradient indicates the direction of the steepest ascent, guiding us on how to adjust the weights and biases to reduce the loss. The chain rule provides a mechanism to compute these gradients efficiently by breaking down the calculation into smaller, manageable steps.\n",
        "How the Chain Rule Works in Backpropagation\n",
        "\n",
        "Imagine a simple neural network with two layers. The chain rule helps us understand how a change in a weight in the first layer affects the final output by considering the intermediate calculations in the second layer. It essentially allows us to trace the impact of a change through the entire network, layer by layer.\n",
        "\n",
        "Mathematical Representation\n",
        "\n",
        "If we have a function f(g(x)), the chain rule states that its derivative with respect to x is:"
      ],
      "metadata": {
        "id": "6mvpVzNfYCXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df/dx = (df/dg) * (dg/dx)"
      ],
      "metadata": {
        "id": "NcmjUjtmYRVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of backpropagation, this translates to:\n",
        "\n",
        "df/dx: The gradient of the loss function with respect to a weight or bias.\n",
        "df/dg: The gradient of the loss function with respect to the output of the current layer.\n",
        "dg/dx: The gradient of the current layer's output with respect to the weight or bias.\n",
        "By repeatedly applying the chain rule for each layer, we can efficiently calculate the gradients needed to update the network's parameters and improve its performance.\n",
        "\n",
        "In simpler terms\n",
        "\n",
        "The chain rule in backpropagation acts like a messenger, relaying information about the error at the output layer back through each layer of the network. It helps the network understand how each weight and bias contributed to the error, enabling it to make the necessary adjustments during training."
      ],
      "metadata": {
        "id": "9oiteNJPYSQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5..\n"
      ],
      "metadata": {
        "id": "_tFLSsrgYWPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "  \"\"\"\n",
        "  Performs forward propagation for a simple neural network with one hidden layer.\n",
        "\n",
        "  Args:\n",
        "    X: The input data.\n",
        "    W1: The weights for the hidden layer.\n",
        "    b1: The biases for the hidden layer.\n",
        "    W2: The weights for the output layer.\n",
        "    b2: The biases for the output layer.\n",
        "\n",
        "  Returns:\n",
        "    The predicted output.\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate the hidden layer output\n",
        "  hidden_layer_input = np.dot(X, W1) + b1\n",
        "  hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "  # Calculate the output layer output\n",
        "  output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
        "  output_layer_output = sigmoid(output_layer_input)\n",
        "\n",
        "  return output_layer_output"
      ],
      "metadata": {
        "id": "gckkGHo_Yduo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Import NumPy: The code begins by importing the NumPy library, which is essential for numerical computations in Python.\n",
        "\n",
        "Sigmoid Function: The sigmoid function is defined to introduce non-linearity into the network. It takes a numerical input and squashes it to a value between 0 and 1.\n",
        "\n",
        "Forward Propagation Function: The forward_propagation function takes the input data (X), weights (W1, W2), and biases (b1, b2) as arguments.\n",
        "\n",
        "Hidden Layer Calculation: It calculates the input to the hidden layer by performing a dot product between the input data and the hidden layer weights, and then adds the hidden layer biases. This result is then passed through the sigmoid activation function to obtain the hidden layer output.\n",
        "\n",
        "Output Layer Calculation: Similarly, the output layer input is calculated using the hidden layer output, output layer weights, and output layer biases. It is also passed through the sigmoid activation function to obtain the final predicted output.\n",
        "\n",
        "Return Output: The function returns the predicted output."
      ],
      "metadata": {
        "id": "nuU_hruuYkpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "# Weights and biases\n",
        "W1 = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
        "b1 = np.array([0.5, 0.5])\n",
        "W2 = np.array([[0.5], [0.5]])\n",
        "b2 = np.array([0.5])\n",
        "\n",
        "# Perform forward propagation\n",
        "output = forward_propagation(X, W1, b1, W2, b2)\n",
        "\n",
        "#To see the output, run the code."
      ],
      "metadata": {
        "id": "7sHLRTYxYqzj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vfQI6R7aYrmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}