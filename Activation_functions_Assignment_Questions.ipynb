{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UExZ6I0L2Toz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Role of Activation Functions in Neural Networks\n",
        "Activation functions are mathematical functions applied to the output of a neuron in a neural network to introduce non-linearity. They determine whether a neuron should be \"activated\" (i.e., produce output) based on its input. Activation functions are crucial for:\n",
        "\n",
        "Introducing Non-Linearity: Without activation functions, neural networks behave like linear models, limiting their ability to model complex relationships.\n",
        "Controlling Output Range: They help in keeping neuron outputs within a defined range, improving numerical stability.\n",
        "Learning Complex Patterns: By introducing non-linearity, activation functions enable neural networks to learn intricate patterns and representations in data.\n",
        "Linear vs. Nonlinear Activation Functions\n",
        "Linear Activation Functions\n",
        "Definition: Linear functions are of the form\n",
        "\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "ð‘Ž\n",
        "ð‘¥\n",
        "+\n",
        "ð‘\n",
        "f(x)=ax+b, where\n",
        "ð‘Ž\n",
        "a and\n",
        "ð‘\n",
        "b are constants.\n",
        "Output: The output is a scaled version of the input.\n",
        "Advantages:\n",
        "Simple to compute.\n",
        "Useful in the output layer for regression tasks (e.g., predicting continuous values).\n",
        "Disadvantages:\n",
        "Linear functions do not introduce non-linearity, meaning multiple layers with linear activations are mathematically equivalent to a single-layer model.\n",
        "Cannot capture complex patterns or decision boundaries.\n",
        "Nonlinear Activation Functions\n",
        "Definition: Nonlinear functions introduce non-linearity into the network, allowing it to model complex relationships.\n",
        "Common Examples:\n",
        "ReLU (Rectified Linear Unit):\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "max\n",
        "â¡\n",
        "(\n",
        "0\n",
        ",\n",
        "ð‘¥\n",
        ")\n",
        "f(x)=max(0,x)\n",
        "Sigmoid:\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ð‘’\n",
        "âˆ’\n",
        "ð‘¥\n",
        "f(x)=\n",
        "1+e\n",
        "âˆ’x\n",
        "\n",
        "1\n",
        "â€‹\n",
        "\n",
        "Tanh:\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "tanh\n",
        "â¡\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "ð‘’\n",
        "ð‘¥\n",
        "âˆ’\n",
        "ð‘’\n",
        "âˆ’\n",
        "ð‘¥\n",
        "ð‘’\n",
        "ð‘¥\n",
        "+\n",
        "ð‘’\n",
        "âˆ’\n",
        "ð‘¥\n",
        "f(x)=tanh(x)=\n",
        "e\n",
        "x\n",
        " +e\n",
        "âˆ’x\n",
        "\n",
        "e\n",
        "x\n",
        " âˆ’e\n",
        "âˆ’x\n",
        "\n",
        "â€‹\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Enable the network to approximate any function (universal approximation property).\n",
        "\n",
        "Allow deeper architectures to learn hierarchical patterns.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Some, like Sigmoid and Tanh, suffer from vanishing gradient problems in deep networks.\n",
        "\n",
        "Others, like ReLU, may cause \"dead neurons\" (neurons stuck at 0 output).\n",
        "Why Nonlinear Activation Functions Are Preferred in Hidden Layers\n",
        "Modeling Complex Patterns:\n",
        "\n",
        "Nonlinear functions allow the network to learn and approximate complex functions and relationships in the data.\n",
        "\n",
        "With non-linearities, each layer extracts increasingly abstract features.\n",
        "\n",
        "Hierarchical Representations:\n",
        "\n",
        "Nonlinear functions enable the stacking of layers to build hierarchical representations of data (e.g., edges in early layers, shapes in middle layers, and objects in later layers).\n",
        "\n",
        "Universal Approximation:\n",
        "\n",
        "A neural network with at least one hidden layer and non-linear activation functions can approximate any continuous function.\n",
        "\n",
        "Breaking Linearity:\n",
        "\n",
        "Without non-linearity, hidden layers collapse into a single linear transformation, limiting the network's expressive power.\n"
      ],
      "metadata": {
        "id": "PSv8yGWA2VzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Sigmoid Activation Function\n",
        "\n",
        "Definition:\n",
        "\n",
        "The Sigmoid activation function is defined as:\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ð‘’\n",
        "âˆ’\n",
        "ð‘¥\n",
        "f(x)=\n",
        "1+e\n",
        "âˆ’x\n",
        "\n",
        "1\n",
        "â€‹\n",
        "\n",
        "It maps input values to a range between\n",
        "0\n",
        "0 and\n",
        "1\n",
        "1.\n",
        "\n",
        "Characteristics:\n",
        "Range:\n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        ")\n",
        "(0,1)\n",
        "S-shaped Curve: It has a smooth, continuous curve.\n",
        "Monotonic: The function is always increasing, making it predictable.\n",
        "Output Interpretation: Often interpreted as probabilities in binary classification tasks.\n",
        "Gradient: The derivative of the Sigmoid function is:\n",
        "ð‘“\n",
        "â€²\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "â‹…\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        ")\n",
        "f\n",
        "â€²\n",
        " (x)=f(x)â‹…(1âˆ’f(x))\n",
        "Common Uses:\n",
        "Used in output layers for binary classification problems (e.g., logistic regression).\n",
        "Suitable for probabilistic outputs because the range is bounded between\n",
        "0\n",
        "0 and\n",
        "1\n",
        "1.\n",
        "Challenges:\n",
        "Vanishing Gradient Problem: For very large or small inputs, the gradient approaches\n",
        "0\n",
        "0, slowing down learning during backpropagation.\n",
        "Non-zero Centered Output: Sigmoid outputs are always positive, which can lead to inefficient gradient updates in optimization.\n",
        "Rectified Linear Unit (ReLU) Activation Function\n",
        "Definition:\n",
        "The ReLU activation function is defined as:\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "max\n",
        "â¡\n",
        "(\n",
        "0\n",
        ",\n",
        "ð‘¥\n",
        ")\n",
        "f(x)=max(0,x)\n",
        "It outputs the input directly if it is positive; otherwise, it outputs\n",
        "0\n",
        "0.\n",
        "\n",
        "Characteristics:\n",
        "Range:\n",
        "[\n",
        "0\n",
        ",\n",
        "âˆž\n",
        ")\n",
        "[0,âˆž)\n",
        "Non-linear: Despite its simplicity, it introduces non-linearity.\n",
        "Piecewise Function: The function is linear for positive inputs and flat for negative inputs.\n",
        "Sparse Activation: Only neurons with positive inputs are activated.\n",
        "Advantages:\n",
        "Computational Efficiency: Easy to compute and highly efficient in large networks.\n",
        "Avoids Vanishing Gradients: For positive inputs, the gradient is\n",
        "1\n",
        "1, which helps maintain gradients during backpropagation.\n",
        "Sparse Representations: Leads to more efficient representations as many neurons output\n",
        "0\n",
        "0.\n",
        "Challenges:\n",
        "Dead Neurons: If weights lead to negative inputs, neurons can get \"stuck\" at\n",
        "0\n",
        "0 and stop learning.\n",
        "Unbounded Output: Can lead to unstable gradients in some cases.\n",
        "Variants:\n",
        "Leaky ReLU: Addresses the dead neuron problem by allowing a small slope for negative inputs:\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "ð‘¥\n",
        "Â if\n",
        "ð‘¥\n",
        ">\n",
        "0\n",
        ",\n",
        "\n",
        "ð›¼\n",
        "ð‘¥\n",
        "Â otherwiseÂ (where\n",
        "ð›¼\n",
        ">\n",
        "0\n",
        ")\n",
        ".\n",
        "f(x)=xÂ ifÂ x>0,Î±xÂ otherwiseÂ (whereÂ Î±>0).\n",
        "Tanh (Hyperbolic Tangent) Activation Function\n",
        "Definition:\n",
        "The Tanh function is defined as:\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "tanh\n",
        "â¡\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "ð‘’\n",
        "ð‘¥\n",
        "âˆ’\n",
        "ð‘’\n",
        "âˆ’\n",
        "ð‘¥\n",
        "ð‘’\n",
        "ð‘¥\n",
        "+\n",
        "ð‘’\n",
        "âˆ’\n",
        "ð‘¥\n",
        "f(x)=tanh(x)=\n",
        "e\n",
        "x\n",
        " +e\n",
        "âˆ’x\n",
        "\n",
        "e\n",
        "x\n",
        " âˆ’e\n",
        "âˆ’x\n",
        "\n",
        "â€‹\n",
        "\n",
        "It maps input values to a range between\n",
        "âˆ’\n",
        "1\n",
        "âˆ’1 and\n",
        "1\n",
        "1.\n",
        "\n",
        "Characteristics:\n",
        "Range:\n",
        "(\n",
        "âˆ’\n",
        "1\n",
        ",\n",
        "1\n",
        ")\n",
        "(âˆ’1,1)\n",
        "Zero-Centered Output: Unlike Sigmoid, Tanh outputs are centered around\n",
        "0\n",
        "0, making gradient updates more efficient.\n",
        "Gradient: The derivative of Tanh is:\n",
        "ð‘“\n",
        "â€²\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "1\n",
        "âˆ’\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "2\n",
        "f\n",
        "â€²\n",
        " (x)=1âˆ’f(x)\n",
        "2\n",
        "\n",
        "Common Uses:\n",
        "Typically used in hidden layers of neural networks when zero-centered outputs are beneficial for optimization.\n"
      ],
      "metadata": {
        "id": "mIzKJMC529Y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\n",
        "\n",
        "\n",
        "1.Introducing Non-Linearity\n",
        "\n",
        "Why It Matters: Neural networks without activation functions are equivalent to linear transformations, regardless of how many layers they have. In such cases, stacking layers would only produce a linear combination of the input features.\n",
        "\n",
        "Impact: Non-linear activation functions allow the model to learn non-linear mappings, which are essential for solving real-world problems like image recognition, natural language processing, and more.\n",
        "\n",
        "2. Enabling Complex Feature Learning\n",
        "\n",
        "Hidden layers extract increasingly abstract features from the input data.\n",
        "\n",
        "For example:\n",
        "\n",
        "In image recognition, early layers might learn edges, while deeper layers identify shapes, objects, or even entire scenes.\n",
        "\n",
        "Activation functions are necessary for these layers to capture the complexity of the patterns.\n",
        "\n",
        "3. Supporting the Universal Approximation Theorem\n",
        "\n",
        "The universal approximation theorem states that a feedforward neural network with at least one hidden layer and non-linear activation functions can approximate any continuous function, given sufficient neurons.\n",
        "\n",
        "Key Insight: Without non-linear activation functions, this theorem would not hold true.\n",
        "\n",
        "4. Hierarchical Representations\n",
        "\n",
        "Activation functions allow data to flow through multiple layers with non-linear transformations, enabling the network to build hierarchical representations.\n",
        "This hierarchical learning is key to understanding relationships in complex data, such as speech, images, or text.\n",
        "\n",
        "5. Controlling the Output Range\n",
        "Activation functions like ReLU, Sigmoid, and Tanh constrain neuron outputs within specific ranges.\n",
        "\n",
        "This:\n",
        "\n",
        "Prevents exploding outputs in deep networks.\n",
        "Keeps gradients stable during backpropagation.\n",
        "\n",
        "6. Facilitating Gradient-Based Optimization\n",
        "Activation functions determine how errors propagate backward during training (backpropagation).\n",
        "\n",
        "By transforming raw outputs, they:\n",
        "\n",
        "Ensure gradients are meaningful for weight updates.\n",
        "\n",
        "Avoid flat gradients in non-active neurons (e.g., in ReLU or Sigmoid).\n",
        "\n",
        "7. Enhancing Model Flexibility\n",
        "Non-linear activation functions give each hidden layer flexibility to transform input data in ways that simple linear transformations cannot.\n",
        "\n",
        "This flexibility is what makes neural networks capable of solving tasks like language translation or medical image analysis.\n",
        "\n",
        "Challenges and Trade-Offs\n",
        "While activation functions are essential, choosing the right activation function is also critical:\n",
        "\n",
        "Vanishing Gradient Problem: Functions like Sigmoid and Tanh can lead to small gradients, especially in deep networks.\n",
        "\n",
        "Dead Neurons: ReLU may cause some neurons to stop learning if their outputs become\n",
        "\n",
        "0\n",
        "\n",
        "0 permanently.\n",
        "\n",
        "Numerical Instability: Unbounded activations like ReLU may lead to large outputs or unstable gradients."
      ],
      "metadata": {
        "id": "q-t6sK5r3ZO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\n",
        "1. Regression Problems\n",
        "Regression involves predicting continuous values, such as house prices, temperatures, or stock prices.\n",
        "\n",
        "Common Activation Functions:\n",
        "Linear Activation Function:\n",
        "\n",
        "Equation:\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "ð‘¥\n",
        "f(x)=x\n",
        "Output Range:\n",
        "(\n",
        "âˆ’\n",
        "âˆž\n",
        ",\n",
        "âˆž\n",
        ")\n",
        "(âˆ’âˆž,âˆž)\n",
        "Why It's Used:\n",
        "In regression, the target variable can take any real value, so no restriction on the output range is needed.\n",
        "The absence of non-linearity ensures no distortion of predictions.\n",
        "Other Variants:\n",
        "\n",
        "ReLU:\n",
        "Sometimes used if the output is constrained to non-negative values (e.g., age, count of items).\n",
        "2. Binary Classification Problems\n",
        "Binary classification involves distinguishing between two classes, such as \"yes\" or \"no\", \"cat\" or \"dog\".\n",
        "\n",
        "Common Activation Functions:\n",
        "\n",
        "Sigmoid Activation Function:\n",
        "\n",
        "Equation:\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ð‘’\n",
        "âˆ’\n",
        "ð‘¥\n",
        "f(x)=\n",
        "1+e\n",
        "âˆ’x\n",
        "\n",
        "1\n",
        "â€‹\n",
        "\n",
        "\n",
        "Output Range:\n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        ")\n",
        "(0,1)\n",
        "Why It's Used:\n",
        "The Sigmoid function outputs probabilities, making it ideal for binary classification.\n",
        "After applying the Sigmoid, the result can be interpreted as the probability of belonging to a specific class.\n",
        "Example:\n",
        "Logistic regression uses Sigmoid as its output activation.\n",
        "Binary Cross-Entropy Loss:\n",
        "\n",
        "Often paired with Sigmoid in binary classification tasks.\n",
        "3. Multi-Class Classification Problems\n",
        "Multi-class classification involves predicting one class from multiple possible classes (e.g., identifying digits 0â€“9).\n",
        "\n",
        "Common Activation Functions:\n",
        "Softmax Activation Function:\n",
        "\n",
        "Equation:\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        "ð‘–\n",
        ")\n",
        "=\n",
        "ð‘’\n",
        "ð‘¥\n",
        "ð‘–\n",
        "âˆ‘\n",
        "ð‘—\n",
        "ð‘’\n",
        "ð‘¥\n",
        "ð‘—\n",
        "f(x\n",
        "i\n",
        "â€‹\n",
        " )=\n",
        "âˆ‘\n",
        "j\n",
        "â€‹\n",
        " e\n",
        "x\n",
        "j\n",
        "â€‹\n",
        "\n",
        "\n",
        "e\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "\n",
        "\n",
        "â€‹\n",
        "\n",
        "where\n",
        "ð‘¥\n",
        "ð‘–\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "  is the input for class\n",
        "ð‘–\n",
        "i.\n",
        "Output Range:\n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        ")\n",
        "(0,1), with the sum of all outputs equal to\n",
        "1\n",
        "1.\n",
        "Why It's Used:\n",
        "Softmax converts raw scores (logits) into probabilities for each class.\n",
        "It ensures outputs are normalized, so the highest probability corresponds to the predicted class.\n",
        "Example:\n",
        "Used in tasks like digit recognition or image classification (e.g., MNIST, CIFAR-10).\n",
        "Categorical Cross-Entropy Loss:\n",
        "\n",
        "Typically paired with Softmax for training multi-class classifiers.\n",
        "4. Multi-Label Classification Problems\n",
        "Multi-label classification involves predicting multiple independent labels (e.g., tagging an image with \"dog\" and \"grass\").\n",
        "\n",
        "Common Activation Functions:\n",
        "Sigmoid Activation Function:\n",
        "\n",
        "Why It's Used:\n",
        "Unlike Softmax, Sigmoid allows each output neuron to act independently, predicting the probability of each label being present.\n",
        "Output:\n",
        "Each neuron outputs a probability between\n",
        "0\n",
        "0 and\n",
        "1\n",
        "1, allowing multiple labels to be selected simultaneously.\n",
        "Binary Cross-Entropy Loss:\n",
        "\n",
        "Often used with Sigmoid for multi-label problems.\n",
        "5. Imbalanced Classification Problems\n",
        "For imbalanced datasets where one class is rare (e.g., fraud detection), you might:\n",
        "\n",
        "Use Sigmoid or Softmax, depending on whether the problem is binary or multi-class.\n",
        "Adjust the decision threshold (e.g., for Sigmoid, predict\n",
        "1\n",
        "1 if\n",
        "ð‘ƒ\n",
        "(\n",
        "ð‘¦\n",
        ")\n",
        ">\n",
        "0.3\n",
        "P(y)>0.3 instead of\n",
        "0.5\n",
        "0.5).\n",
        "6. Problems with Specific Constraints\n",
        "Some problems require outputs in a specific range or format:\n",
        "\n",
        "Non-Negative Outputs:\n",
        "Use ReLU to ensure outputs are always\n",
        "â‰¥\n",
        "0\n",
        "â‰¥0.\n",
        "Normalized Outputs:\n",
        "Use Softmax when outputs must represent probabilities summing to\n",
        "1\n",
        "1."
      ],
      "metadata": {
        "id": "QGBmT_lt4Fmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\n",
        "\n",
        "Experiment Setup\n",
        "\n",
        "1. Dataset\n",
        "Use a synthetic 2D dataset such as Moons or Circles from the sklearn.datasets library.\n",
        "\n",
        "These datasets are non-linearly separable, making them ideal for observing the effect of activation functions.\n",
        "\n",
        "2. Neural Network Architecture\n",
        "\n",
        "A simple feedforward network with:\n",
        "\n",
        "Input layer (2 neurons, one for each feature).\n",
        "\n",
        "2 hidden layers (with 10 neurons each).\n",
        "\n",
        "Output layer (1 neuron for binary classification).\n",
        "\n",
        "3. Activation Functions\n",
        "\n",
        "Test the following activation functions in the hidden layers:\n",
        "ReLU\n",
        "Sigmoid\n",
        "Tanh\n",
        "Use Sigmoid in the output layer for binary classification.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "Accuracy: Measure classification performance.\n",
        "\n",
        "Loss Curve: Observe the loss reduction during training.\n",
        "\n",
        "Convergence Speed: Compare how quickly the network converges to a stable solution.\n",
        "\n",
        "Implementation Plan\n",
        "\n",
        "Here's the Python code outline:"
      ],
      "metadata": {
        "id": "uZDIbk4E4lp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define a function to create and train models with different activation functions\n",
        "def train_model(activation_function):\n",
        "    # Build the model\n",
        "    model = Sequential([\n",
        "        Dense(10, input_dim=2, activation=activation_function),  # Hidden layer 1\n",
        "        Dense(10, activation=activation_function),               # Hidden layer 2\n",
        "        Dense(1, activation='sigmoid')                           # Output layer\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0, validation_split=0.2)\n",
        "\n",
        "    # Evaluate the model\n",
        "    train_accuracy = model.evaluate(X_train, y_train, verbose=0)[1]\n",
        "    test_accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "\n",
        "    return history, train_accuracy, test_accuracy\n",
        "\n",
        "# Compare activation functions\n",
        "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
        "results = {}\n",
        "\n",
        "for activation in activation_functions:\n",
        "    print(f\"Training with {activation} activation...\")\n",
        "    history, train_acc, test_acc = train_model(activation)\n",
        "    results[activation] = {'history': history, 'train_acc': train_acc, 'test_acc': test_acc}\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 6))\n",
        "for activation in activation_functions:\n",
        "    plt.plot(results[activation]['history'].history['loss'], label=f\"{activation} (Train Loss)\")\n",
        "    plt.plot(results[activation]['history'].history['val_loss'], label=f\"{activation} (Val Loss)\", linestyle=\"--\")\n",
        "\n",
        "plt.title(\"Loss Curves for Different Activation Functions\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print final accuracies\n",
        "for activation in activation_functions:\n",
        "    print(f\"{activation.capitalize()} - Train Accuracy: {results[activation]['train_acc']:.4f}, \"\n",
        "          f\"Test Accuracy: {results[activation]['test_acc']:.4f}\")\n"
      ],
      "metadata": {
        "id": "yE5Ay7PM5SV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Observations\n",
        "\n",
        "1. ReLU\n",
        "Convergence: Fast convergence due to its simplicity and efficient gradient propagation.\n",
        "\n",
        "Performance: Likely to achieve good accuracy, as it avoids vanishing gradients and facilitates sparse activations.\n",
        "\n",
        "Challenges: Potential \"dead neurons\" (neurons stuck at 0).\n",
        "\n",
        "2. Sigmoid\n",
        "\n",
        "Convergence: Slower convergence due to vanishing gradients, especially in deeper networks.\n",
        "\n",
        "Performance: Moderate accuracy, but gradients near 0 for extreme inputs may hinder learning.\n",
        "\n",
        "Challenges: Non-zero-centered outputs may slow down optimization.\n",
        "\n",
        "3. Tanh\n",
        "\n",
        "Convergence: Slower than ReLU but faster than Sigmoid because it is zero-centered.\n",
        "\n",
        "Performance: Better accuracy than Sigmoid in hidden layers due to its centered output.\n",
        "\n",
        "Challenges: Still suffers from vanishing gradients for large inputs.\n",
        "\n",
        "Analysis\n",
        "\n",
        "Compare the loss curves: Faster convergence and lower final loss indicate better activation function performance.\n",
        "\n",
        "Compare train/test accuracies: High test accuracy with low train-test gap suggests a well-generalized model.\n",
        "\n",
        "Consider challenges like dead neurons (ReLU) or slow convergence (Sigmoid/Tanh)."
      ],
      "metadata": {
        "id": "YMu14j_g5CPn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FNL8wojO49-2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}